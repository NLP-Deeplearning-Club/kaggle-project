{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM + Attention + Preprocessing\n",
    "- Based on previous LSTM + Attention Model, add some preprocessings\n",
    "    - Use only 10 labels + silence + unknown instead of all 32 labels\n",
    "    - Apply preprocessing for input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the words that we predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 possible labels : ['silence', 'unknown', 'yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go']\n"
     ]
    }
   ],
   "source": [
    "wanted_words = ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go']\n",
    "possible_labels = ['silence', 'unknown'] + wanted_words\n",
    "\n",
    "print(\"{} possible labels : {}\".format(len(possible_labels), possible_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and save wav data functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from scipy.io import wavfile\n",
    "import numpy as np\n",
    "\n",
    "def load_wav_file(filename):\n",
    "    \"\"\"Loads an audio file and returns a float PCM-encoded array of samples.\n",
    "  \n",
    "    Args:\n",
    "        filename: Path to the .wav file to load.\n",
    "    Returns:\n",
    "        Numpy array holding the sample data as floats between -1.0 and 1.0.\n",
    "    \"\"\"\n",
    "    \n",
    "    _, wav = wavfile.read(str(filename))\n",
    "    wav = wav.astype(np.float32) / np.iinfo(np.int16).max\n",
    "    return wav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a file to have a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  9.15555283e-05,   3.05185094e-05,   1.83111057e-04, ...,\n",
       "        -3.05185094e-05,  -9.15555283e-05,   1.22074038e-04], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "load_wav_file(os.path.join(os.getcwd(),'dataset','train','audio','bed','00176480_nohash_0.wav'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################ Train & Val & Test Split ################\n",
    "validation_percentage = 10\n",
    "testing_percentage    = 10\n",
    "\n",
    "################ Silence and Unknown Ratio ################\n",
    "silence_percentage = 10.0\n",
    "unknown_percentage = 10.0\n",
    "\n",
    "##################### Labels #############################\n",
    "label_count = len(possible_labels)\n",
    "\n",
    "###################### Audio #############################\n",
    "sample_rate = 16000\n",
    "clip_duration_ms = 1000\n",
    "desired_samples = 16000\n",
    "\n",
    "################ Background Noise ########################\n",
    "background_volume_range = 0.1\n",
    "background_frequency = 0.1\n",
    "\n",
    "################### Wav Shifting #########################\n",
    "time_shift = int(desired_samples/4)\n",
    "\n",
    "############### MFCC Processing Params ###################\n",
    "# Duration of frequency analysis window.\n",
    "window_size_ms = 30.0\n",
    "# How far to move in time between frequency windows.\n",
    "window_stride_ms = 10.0\n",
    "# Number of frequency bins to use for analysis.\n",
    "dct_coefficient_count = 26\n",
    "\n",
    "################# Keras Model Params #####################\n",
    "max_len = 99\n",
    "embed_dim = dct_coefficient_count\n",
    "MODEL_TYPE = \"bi-lstm\"\n",
    "ATTENTION = True\n",
    "LEARN_RATE = 0.00001\n",
    "BATCH_SIZE = batch_size = 140\n",
    "INPUT_SHAPE = [max_len, embed_dim]\n",
    "EPOCHE = 6\n",
    "NUM_HIDDEN = 100\n",
    "TIME_STEPS = max_len\n",
    "SINGLE_ATTENTION_VECTOR = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into train, validation and test based on its name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tell if a file is belonging to train, val or test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import re\n",
    "import hashlib\n",
    "\n",
    "from tensorflow.python.util import compat\n",
    "\n",
    "MAX_NUM_WAVS_PER_CLASS = 2 ** 27 - 1  # ~134M\n",
    "\n",
    "def which_set(filename, validation_percentage=validation_percentage, testing_percentage=testing_percentage):\n",
    "    \"\"\"Determines which data partition the file should belong to.\n",
    "      \n",
    "    Args:\n",
    "        filename: File path of the data sample.\n",
    "        validation_percentage: How much of the data set to use for validation.\n",
    "        testing_percentage: How much of the data set to use for testing.\n",
    "    Returns:\n",
    "        String, one of 'training', 'validation', or 'testing'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get hash value based on filename\n",
    "    base_name = os.path.basename(filename)\n",
    "    hash_name = re.sub(r'_nohash_.*$', '', base_name)\n",
    "    hash_name_hashed = hashlib.sha1(compat.as_bytes(hash_name)).hexdigest()\n",
    "    \n",
    "    # Turn hash value -> percentage\n",
    "    percentage_hash = int(hash_name_hashed, 16) \\\n",
    "        % (MAX_NUM_WAVS_PER_CLASS + 1) * (100.0 / MAX_NUM_WAVS_PER_CLASS)\n",
    "    \n",
    "    # Assign which data set it belongs to by hash percentage\n",
    "    if percentage_hash < validation_percentage:\n",
    "        result = 'validation'\n",
    "    elif percentage_hash < testing_percentage + validation_percentage:\n",
    "        result = 'testing'\n",
    "    else:\n",
    "        result = 'training'\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give a random file name and see its belonging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'training'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "which_set(os.path.join(os.getcwd(),'dataset','train','audio','bed','00176480_nohash_0.wav'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepares a list of the samples organized by set and label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_seed = 1234\n",
    "data_dir = os.path.join(os.getcwd(),'dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import glob\n",
    "\n",
    "def prepare_data_index():\n",
    "\n",
    "    random.seed(random_seed)\n",
    "    wanted_words_index = {}\n",
    "    for index, word in enumerate(wanted_words):\n",
    "        wanted_words_index[word] = index + 2\n",
    "\n",
    "    data_index = {'validation': [], 'testing': [],\n",
    "                           'training': []}\n",
    "    unknown_index = {'validation': [], 'testing': [],\n",
    "                         'training': []}\n",
    "    all_words = {}\n",
    "\n",
    "    # Look through all the subfolders to find audio samples\n",
    "    search_path = os.path.join(data_dir,'train','audio','*','*.wav')\n",
    "\n",
    "    for wav_path in glob.glob(search_path):\n",
    "        (_, word) = os.path.split(os.path.dirname(wav_path))\n",
    "        word = word.lower()\n",
    "        if word == '_background_noise_':\n",
    "            continue\n",
    "        all_words[word] = True\n",
    "        set_index = which_set(wav_path)\n",
    "        if word in wanted_words_index:\n",
    "            data_index[set_index].append({'label': word, 'file': wav_path})\n",
    "        else:\n",
    "            unknown_index[set_index].append({'label': word, 'file': wav_path})\n",
    "\n",
    "    # We need an arbitrary file to load as the input for the silence samples.\n",
    "    # It's multiplied by zero later, so the content doesn't matter.\n",
    "\n",
    "    silence_wav_path = data_index['training'][0]['file']\n",
    "    for set_index in ['validation', 'testing', 'training']:\n",
    "        set_size = len(data_index[set_index])\n",
    "        silence_size = int(math.ceil(set_size * silence_percentage / 100))\n",
    "        for _ in range(silence_size):\n",
    "            data_index[set_index].append({'label': 'silence',\n",
    "                    'file': silence_wav_path})\n",
    "\n",
    "        # Pick some unknowns to add to each partition of the data set.\n",
    "        random.shuffle(unknown_index[set_index])\n",
    "        unknown_size = int(math.ceil(set_size * unknown_percentage / 100))\n",
    "        data_index[set_index].extend((unknown_index[set_index])[:unknown_size])\n",
    "        \n",
    "        \n",
    "    # Make sure the ordering is random.\n",
    "    for set_index in ['validation', 'testing', 'training']:\n",
    "        random.shuffle(data_index[set_index])\n",
    "\n",
    "    # Prepare the rest of the result data structure.\n",
    "    words_list = possible_labels\n",
    "    word_to_index = {}\n",
    "    for word in all_words:\n",
    "        if word in wanted_words_index:\n",
    "            word_to_index[word] = wanted_words_index[word]\n",
    "        else:\n",
    "            word_to_index[word] = 1\n",
    "    word_to_index['silence'] = 0\n",
    "    \n",
    "    return data_index, word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'silence', 'file': '/Users/yufan/Documents/workspace/kaggle-project/dataset/train/audio/right/988e2f9a_nohash_0.wav'}\n",
      "{'label': 'right', 'file': '/Users/yufan/Documents/workspace/kaggle-project/dataset/train/audio/right/68dd409e_nohash_0.wav'}\n",
      "{'label': 'right', 'file': '/Users/yufan/Documents/workspace/kaggle-project/dataset/train/audio/right/ab7b5acd_nohash_1.wav'}\n"
     ]
    }
   ],
   "source": [
    "data_index, word_to_index = prepare_data_index()\n",
    "print(data_index['training'][0])\n",
    "print(data_index['testing'][0])\n",
    "print(data_index['validation'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 22246, Test 3081, Validation 3093.\n"
     ]
    }
   ],
   "source": [
    "print(\"Train {}, Test {}, Validation {}.\".format( \\\n",
    "    len(data_index['training']), len(data_index['testing']), len(data_index['validation'] ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right ->  7\n",
      "eight ->  1\n",
      "cat ->  1\n",
      "tree ->  1\n",
      "bed ->  1\n",
      "happy ->  1\n",
      "go ->  11\n",
      "dog ->  1\n",
      "no ->  3\n",
      "wow ->  1\n",
      "nine ->  1\n",
      "left ->  6\n",
      "stop ->  10\n",
      "three ->  1\n",
      "sheila ->  1\n",
      "one ->  1\n",
      "bird ->  1\n",
      "zero ->  1\n",
      "seven ->  1\n",
      "up ->  4\n",
      "marvin ->  1\n",
      "two ->  1\n",
      "house ->  1\n",
      "down ->  5\n",
      "six ->  1\n",
      "yes ->  2\n",
      "on ->  8\n",
      "five ->  1\n",
      "off ->  9\n",
      "four ->  1\n",
      "silence ->  0\n"
     ]
    }
   ],
   "source": [
    "for word in word_to_index:\n",
    "    print(word, '-> ', word_to_index[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare background data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_background_data():\n",
    "\n",
    "    background_data = []\n",
    "    background_wavfiles = glob.glob(os.path.join(data_dir,'train','audio','_background_noise_','*.wav'))\n",
    "\n",
    "    for wavfile in background_wavfiles:\n",
    "        wav = load_wav_file(wavfile)\n",
    "        background_data.append(wav)\n",
    "    \n",
    "    return background_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/scipy/io/wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  WavFileWarning)\n"
     ]
    }
   ],
   "source": [
    "background_data = prepare_background_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Background data size: 6\n"
     ]
    }
   ],
   "source": [
    "print(\"Background data size: {}\".format(len(background_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "# Preprocessing and Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make wav same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def desired_samples_wav(wav, desired_samples=desired_samples):\n",
    "    wav_length = wav.shape[0]\n",
    "    \n",
    "    if wav_length < desired_samples:\n",
    "        # Pad 0 at the end\n",
    "        desired_wav = np.lib.pad(wav, (0, desired_samples-wav_length), mode='constant')\n",
    "    elif wav_length > desired_samples:\n",
    "        # Random choose a range from the data\n",
    "        start = np.random.randint(0, wav_length-desired_samples)\n",
    "        desired_wav = wav[start:start+desired_samples]\n",
    "    else:\n",
    "        desired_wav = wav\n",
    "        \n",
    "    return desired_wav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set all silence's volumn as all zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def silence_as_zero(wav, label):\n",
    "    \n",
    "    if label == 'silence':\n",
    "        # If silence, set all volumn as 0\n",
    "        volume_scale = 0\n",
    "    else:\n",
    "        volume_scale = 1\n",
    "    \n",
    "    return np.multiply(wav, volume_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File to be processed: /Users/yufan/Documents/workspace/kaggle-project/dataset/train/audio/right/988e2f9a_nohash_0.wav Label: silence\n",
      "Before processing: [-0.00064089 -0.00091556 -0.00088504 ..., -0.0004883  -0.00054933\n",
      " -0.00085452]\n",
      "After processing: [-0. -0. -0. ..., -0. -0. -0.]\n"
     ]
    }
   ],
   "source": [
    "wav_filename = data_index['training'][0]['file']\n",
    "wav_label = data_index['training'][0]['label']\n",
    "print(\"File to be processed:\", wav_filename, 'Label:', wav_label)\n",
    "wav_data = load_wav_file(wav_filename)\n",
    "print(\"Before processing:\", wav_data)\n",
    "procssed_wav_data = silence_as_zero(wav_data, wav_label)\n",
    "print(\"After processing:\", procssed_wav_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shift the sample's start position, and pad any gaps with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shift_and_pad_zeros(wav, time_shift=time_shift):\n",
    "    \n",
    "    wav_length = wav.shape[0]\n",
    "    \n",
    "    if time_shift > 0:\n",
    "        time_shift_amount = np.random.randint(-time_shift, time_shift)\n",
    "    else:\n",
    "        time_shift_amount = 0\n",
    "    if time_shift_amount > 0:\n",
    "        shifted_wav = np.lib.pad(wav, (0, time_shift_amount), mode='constant')\n",
    "    else:\n",
    "        shifted_wav = np.lib.pad(wav, (-time_shift_amount, 0), mode='constant')\n",
    "    \n",
    "    return shifted_wav[:wav_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File to be processed: /Users/yufan/Documents/workspace/kaggle-project/dataset/train/audio/right/988e2f9a_nohash_0.wav Label: silence\n",
      "Before processing: [-0.00064089 -0.00091556 -0.00088504 ..., -0.0004883  -0.00054933\n",
      " -0.00085452]\n",
      "After processing: [-0.00064089 -0.00091556 -0.00088504 ..., -0.0004883  -0.00054933\n",
      " -0.00085452]\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "wav_filename = data_index['training'][0]['file']\n",
    "wav_label = data_index['training'][0]['label']\n",
    "print(\"File to be processed:\", wav_filename, 'Label:', wav_label)\n",
    "wav_data = load_wav_file(wav_filename)\n",
    "print(\"Before processing:\", wav_data)\n",
    "procssed_wav_data = shift_and_pad_zeros(wav_data, 3000)\n",
    "print(\"After processing:\", procssed_wav_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mix background noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/scipy/io/wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  WavFileWarning)\n"
     ]
    }
   ],
   "source": [
    "background_data = prepare_background_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mix_background_noise(wav, \n",
    "                         background_volume_range=background_volume_range, \n",
    "                         background_frequency=background_frequency,\n",
    "                         background_data=background_data):\n",
    "    \n",
    "    wav_length = wav.shape[0]\n",
    "    wav = wav.reshape(wav_length, 1)\n",
    "    \n",
    "    # Random choose a background data\n",
    "    background_index = np.random.randint(len(background_data))\n",
    "    background_samples = background_data[background_index]\n",
    "    \n",
    "    # Random shift the background data\n",
    "    background_offset = np.random.randint(\n",
    "        0, len(background_samples) - desired_samples)\n",
    "    background_clipped = background_samples[background_offset:(background_offset + desired_samples)]\n",
    "    background_reshaped = background_clipped.reshape([desired_samples, 1])\n",
    "    \n",
    "    # Random choose add background noise or not\n",
    "    if np.random.uniform(0, 1) < background_frequency:\n",
    "        background_volume = np.random.uniform(0, background_volume_range)\n",
    "    else:\n",
    "        background_volume = 0\n",
    "    background_noise = np.multiply(background_reshaped, background_volume)\n",
    "    wav_with_noise = background_noise + wav   \n",
    "    \n",
    "    # Clip by -1, 1\n",
    "    background_clamp = np.clip(wav_with_noise, -1.0, 1.0)\n",
    "    \n",
    "    return wav_with_noise.reshape(wav_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00027321, -0.00068277, -0.00100905, ..., -0.00067322,\n",
       "       -0.00061243, -0.00142888], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mix_background_noise(wav_data, 0.1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def change_volume(wav, min_v=0.7, max_v=1.3):\n",
    "    ratio = np.random.uniform(min_v, max_v)\n",
    "    return np.multiply(wav, ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MFCC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MFCC Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window_size_samples = int(sample_rate * window_size_ms / 1000)\n",
    "window_stride_samples = int(sample_rate * window_stride_ms / 1000)\n",
    "length_minus_window = desired_samples - window_size_samples\n",
    "if length_minus_window < 0:\n",
    "    spectrogram_length = 0\n",
    "else:\n",
    "    spectrogram_length = 1 + int(length_minus_window\n",
    "            / window_stride_samples)\n",
    "fingerprint_size = dct_coefficient_count * spectrogram_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import python_speech_features\n",
    "\n",
    "def mfcc(wav, sample_rate=desired_samples, **kwargs):\n",
    "    \"\"\"从读出的音频数据中算出mfcc,具体可以看python_speech_features的文档\n",
    "\n",
    "    Parameters:\n",
    "        wav (np.ndarray): - 指明音频的振幅序列\n",
    "        sample_rate (int): - 指明抽样率\n",
    "        numcep (int): - 指明返回的倒数数量,默认为13\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: - mfcc强度(二维)组成的元组,shape为(times.shape,numcep)\n",
    "    \"\"\"\n",
    "    return python_speech_features.mfcc(wav, sample_rate, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 26)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfcc(wav_data, numcep = 26).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(fname, label, mode='train'):\n",
    "    wav = load_wav_file(fname)\n",
    "    wav = desired_samples_wav(wav)\n",
    "    if mode=='train':\n",
    "        wav = silence_as_zero(wav, label)\n",
    "        wav = shift_and_pad_zeros(wav)\n",
    "        wav = change_volume(wav)\n",
    "        wav = mix_background_noise(wav)\n",
    "    return mfcc(wav,numcep=dct_coefficient_count), word_to_index[label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "itr = 0\n",
    "all_data_index = data_index['training'] + data_index['testing'] + data_index['validation']\n",
    "\n",
    "def all_data_generator(batch_size=batch_size):\n",
    "    \n",
    "    while 1:\n",
    "        global itr\n",
    "        if itr + batch_size >= len(all_data_index):\n",
    "            itr = 0\n",
    "\n",
    "        X_batches = []\n",
    "        Y_batches = []\n",
    "\n",
    "        for i in range(itr, itr+batch_size):\n",
    "            fname = all_data_index[i]['file']\n",
    "            label = all_data_index[i]['label']\n",
    "            X_input, Y_input = preprocess(fname, label)\n",
    "            X_batches.append(X_input)\n",
    "            Y_batches.append(Y_input)\n",
    "            \n",
    "        itr = itr + batch_size\n",
    "\n",
    "        yield np.array(X_batches), to_categorical(np.array(Y_batches).reshape(batch_size, 1), num_classes=label_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_itr = 0\n",
    "train_data_index = data_index['training']\n",
    "\n",
    "def train_generator(batch_size=batch_size):\n",
    "    \n",
    "    while 1:\n",
    "        global train_itr\n",
    "        if train_itr + batch_size >= len(train_data_index):\n",
    "            train_itr = 0\n",
    "\n",
    "        X_batches = []\n",
    "        Y_batches = []\n",
    "\n",
    "        for i in range(train_itr, train_itr+batch_size):\n",
    "            fname = train_data_index[i]['file']\n",
    "            label = train_data_index[i]['label']\n",
    "            X_input, Y_input = preprocess(fname, label)\n",
    "            X_batches.append(X_input)\n",
    "            Y_batches.append(Y_input)\n",
    "            \n",
    "        train_itr = train_itr + batch_size\n",
    "\n",
    "        yield np.array(X_batches), to_categorical(np.array(Y_batches).reshape(batch_size, 1), num_classes=label_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_itr = 0\n",
    "val_data_index = data_index['validation']\n",
    "\n",
    "def val_generator(batch_size=batch_size):\n",
    "    \n",
    "    while 1:\n",
    "        global val_itr\n",
    "        if val_itr + batch_size >= len(val_data_index):\n",
    "            val_itr = 0\n",
    "\n",
    "        X_batches = []\n",
    "        Y_batches = []\n",
    "\n",
    "        for i in range(val_itr, val_itr+batch_size):\n",
    "            fname = val_data_index[i]['file']\n",
    "            label = val_data_index[i]['label']\n",
    "            X_input, Y_input = preprocess(fname, label, mode='val')\n",
    "            X_batches.append(X_input)\n",
    "            Y_batches.append(Y_input)\n",
    "            \n",
    "        val_itr = val_itr + batch_size\n",
    "\n",
    "        yield np.array(X_batches), to_categorical(np.array(Y_batches).reshape(batch_size, 1), num_classes=label_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data_index = data_index['testing']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "# Build Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, GRU, SimpleRNN, GlobalAveragePooling1D, AveragePooling1D, Activation, Bidirectional\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.layers import merge\n",
    "from keras.layers.core import *\n",
    "from keras.models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/anaconda/lib/python3.6/site-packages/Keras-2.0.8-py3.6.egg/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:30: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    }
   ],
   "source": [
    "def attention_3d_block(inputs):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Reshape((input_dim, TIME_STEPS))(a)\n",
    "    a = Dense(TIME_STEPS, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n",
    "    return output_attention_mul\n",
    "\n",
    "inputs = Input(shape=(INPUT_SHAPE))\n",
    "# RNN Layer\n",
    "if MODEL_TYPE == 'rnn':\n",
    "    rnn_out = SimpleRNN(NUM_HIDDEN, return_sequences=True)(inputs)\n",
    "elif MODEL_TYPE == 'gru':\n",
    "    rnn_out = GRU(NUM_HIDDEN, return_sequences=True)(inputs)\n",
    "elif MODEL_TYPE == 'lstm':\n",
    "    rnn_out = LSTM(NUM_HIDDEN, return_sequences=True)(inputs)\n",
    "elif MODEL_TYPE == 'bi-lstm':\n",
    "    rnn_out = Bidirectional(LSTM(NUM_HIDDEN, return_sequences=True))(inputs)\n",
    "else:\n",
    "    raise NameError(\"Unsupported model type\")\n",
    "# Attention Layer\n",
    "attention_mul = attention_3d_block(rnn_out)\n",
    "attention_mul = Flatten()(attention_mul)\n",
    "output = Dense(label_count, activation='softmax')(attention_mul)\n",
    "model = Model(input=[inputs], output=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 99, 26)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 99, 200)      101600      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "permute_2 (Permute)             (None, 200, 99)      0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 200, 99)      0           permute_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 200, 99)      9900        reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_vec (Permute)         (None, 99, 200)      0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_mul (Merge)           (None, 99, 200)      0           bidirectional_2[0][0]            \n",
      "                                                                 attention_vec[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 19800)        0           attention_mul[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 12)           237612      flatten_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 349,112\n",
      "Trainable params: 349,112\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "203/203 [==============================] - 259s - loss: 1.4119 - acc: 0.5729   \n",
      "Epoch 2/6\n",
      "203/203 [==============================] - 262s - loss: 0.6583 - acc: 0.7985   \n",
      "Epoch 3/6\n",
      "203/203 [==============================] - 335s - loss: 0.4852 - acc: 0.8535   \n",
      "Epoch 4/6\n",
      "203/203 [==============================] - 324s - loss: 0.3964 - acc: 0.8818   \n",
      "Epoch 5/6\n",
      "203/203 [==============================] - 326s - loss: 0.3417 - acc: 0.8976   \n",
      "Epoch 6/6\n",
      "203/203 [==============================] - 327s - loss: 0.3075 - acc: 0.9082   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nmodel.fit_generator(generator = train_generator(), \\n                    steps_per_epoch = len(train_data_index)//batch_size,\\n                    validation_data = val_generator(),\\n                    validation_steps = len(val_data_index)//batch_size,\\n                    epochs=EPOCHE)\\n'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model on all dataset\n",
    "model.fit_generator(generator = all_data_generator(), \n",
    "                    steps_per_epoch = len(all_data_index)//batch_size,\n",
    "                    # validation_data = val_generator(),\n",
    "                    # validation_steps = len(val_data_index)//batch_size,\n",
    "                    epochs=EPOCHE)\n",
    "\n",
    "# Train model on train | test | val\n",
    "'''\n",
    "model.fit_generator(generator = train_generator(), \n",
    "                    steps_per_epoch = len(train_data_index)//batch_size,\n",
    "                    validation_data = val_generator(),\n",
    "                    validation_steps = len(val_data_index)//batch_size,\n",
    "                    epochs=EPOCHE)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See how it works on our split test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/3093\n",
      "Step 100/3093\n",
      "Step 200/3093\n",
      "Step 300/3093\n",
      "Step 400/3093\n",
      "Step 500/3093\n",
      "Step 600/3093\n",
      "Step 700/3093\n",
      "Step 800/3093\n",
      "Step 900/3093\n",
      "Step 1000/3093\n",
      "Step 1100/3093\n",
      "Step 1200/3093\n",
      "Step 1300/3093\n",
      "Step 1400/3093\n",
      "Step 1500/3093\n",
      "Step 1600/3093\n",
      "Step 1700/3093\n",
      "Step 1800/3093\n",
      "Step 1900/3093\n",
      "Step 2000/3093\n",
      "Step 2100/3093\n",
      "Step 2200/3093\n",
      "Step 2300/3093\n",
      "Step 2400/3093\n",
      "Step 2500/3093\n",
      "Step 2600/3093\n",
      "Step 2700/3093\n",
      "Step 2800/3093\n",
      "Step 2900/3093\n",
      "Step 3000/3093\n",
      "Correct Prediction: 2476/3093\n"
     ]
    }
   ],
   "source": [
    "test_data_index = data_index['validation']\n",
    "correct_count = 0\n",
    "\n",
    "for i in range(len(test_data_index)):\n",
    "    fname = test_data_index[i]['file']\n",
    "    label = test_data_index[i]['label']\n",
    "    mfcc_, _ = preprocess(fname, label, mode='test')\n",
    "    predictions = model.predict(mfcc_[np.newaxis,:,:])\n",
    "    word = possible_labels[np.argmax( predictions[0] )]\n",
    "    if word == label:\n",
    "        correct_count = correct_count + 1\n",
    "    if i % 100 == 0:\n",
    "        print(\"Step {}/{}\".format(i, len(test_data_index)))\n",
    "        \n",
    "print('Correct Prediction: {}/{}'.format(correct_count, len(test_data_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob    \n",
    "import pandas as pd\n",
    "\n",
    "test_files = glob.glob(os.path.join(data_dir,'test','audio','*.wav'))\n",
    "test_filenames = [os.path.basename(f) for f in test_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/yufan/Documents/workspace/kaggle-project/dataset/test/audio/clip_667ea2de0.wav'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 0/158538\n",
      "Predicting 3000/158538\n",
      "Predicting 6000/158538\n",
      "Predicting 9000/158538\n",
      "Predicting 12000/158538\n",
      "Predicting 15000/158538\n",
      "Predicting 18000/158538\n",
      "Predicting 21000/158538\n",
      "Predicting 24000/158538\n",
      "Predicting 27000/158538\n",
      "Predicting 30000/158538\n",
      "Predicting 33000/158538\n",
      "Predicting 36000/158538\n",
      "Predicting 39000/158538\n",
      "Predicting 42000/158538\n",
      "Predicting 45000/158538\n",
      "Predicting 48000/158538\n",
      "Predicting 51000/158538\n",
      "Predicting 54000/158538\n",
      "Predicting 57000/158538\n",
      "Predicting 60000/158538\n",
      "Predicting 63000/158538\n",
      "Predicting 66000/158538\n",
      "Predicting 69000/158538\n",
      "Predicting 72000/158538\n",
      "Predicting 75000/158538\n",
      "Predicting 78000/158538\n",
      "Predicting 81000/158538\n",
      "Predicting 84000/158538\n",
      "Predicting 87000/158538\n",
      "Predicting 90000/158538\n",
      "Predicting 93000/158538\n",
      "Predicting 96000/158538\n",
      "Predicting 99000/158538\n",
      "Predicting 102000/158538\n",
      "Predicting 105000/158538\n",
      "Predicting 108000/158538\n",
      "Predicting 111000/158538\n",
      "Predicting 114000/158538\n",
      "Predicting 117000/158538\n",
      "Predicting 120000/158538\n",
      "Predicting 123000/158538\n",
      "Predicting 126000/158538\n",
      "Predicting 129000/158538\n",
      "Predicting 132000/158538\n",
      "Predicting 135000/158538\n",
      "Predicting 138000/158538\n",
      "Predicting 141000/158538\n",
      "Predicting 144000/158538\n",
      "Predicting 147000/158538\n",
      "Predicting 150000/158538\n",
      "Predicting 153000/158538\n",
      "Predicting 156000/158538\n",
      "Finish predicting.....\n",
      "Writing to file.....\n"
     ]
    }
   ],
   "source": [
    "num_test = len(test_files)\n",
    "submission = []\n",
    "\n",
    "for i in range(num_test):\n",
    "    wav_mfcc, _ = preprocess(test_files[i], 'yes', mode='test') # Give a random label but this 'yes' is never used\n",
    "    label = possible_labels[np.argmax( model.predict(wav_mfcc[np.newaxis,:,:]) )]\n",
    "    submission.append([test_filenames[i], label])\n",
    "    if i % 3000 == 0:\n",
    "        print(\"Predicting {}/{}\".format(i, num_test))\n",
    "        \n",
    "print(\"Finish predicting.....\")\n",
    "\n",
    "submissionDF = pd.DataFrame(submission, columns=['fname','label'])\n",
    "\n",
    "print(\"Writing to file.....\")\n",
    "submissionDF.to_csv(\"submission-bi-lstm-attention-prepropcessing-1.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test = len(test_files)\n",
    "\n",
    "test_mfcc_batch = []\n",
    "\n",
    "with open(\"submission.csv\", \"w\") as file:\n",
    "    file.write('fname,label\\n')\n",
    "    for i in range(num_test):\n",
    "        wav_mfcc, _ = preprocess(test_files[i], 'yes', mode='train')\n",
    "        label = possible_labels[np.argmax( model.predict(wav_mfcc[np.newaxis,:,:]) )]\n",
    "        file.write(test_filenames[i]+','+label+'\\n')\n",
    "        if i % 3000 == 0:\n",
    "            print(\"Predicting {}/{}\".format(i, num_test))\n",
    "            \n",
    "print(\"Done....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clip_667ea2de0.wav</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>clip_4e1d2a516.wav</td>\n",
       "      <td>go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>clip_4746c1f34.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>clip_016404de8.wav</td>\n",
       "      <td>on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>clip_dcb5708d7.wav</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>clip_af77b5fb6.wav</td>\n",
       "      <td>on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>clip_a78b014de.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>clip_25f0146ae.wav</td>\n",
       "      <td>silence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>clip_f109a09ec.wav</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>clip_160d3ee69.wav</td>\n",
       "      <td>silence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>clip_f07c5d227.wav</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>clip_fb1d0cdb0.wav</td>\n",
       "      <td>go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>clip_d9baf3376.wav</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>clip_761388cd6.wav</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>clip_0b0070730.wav</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>clip_0033db83b.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>clip_2c1e066c9.wav</td>\n",
       "      <td>silence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>clip_542857c3e.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>clip_54a4ab635.wav</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>clip_0ee7bd486.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>clip_82509c6f5.wav</td>\n",
       "      <td>off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>clip_ca40c9a3c.wav</td>\n",
       "      <td>on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>clip_262464bbc.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>clip_5d5805dda.wav</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>clip_d27c6b701.wav</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>clip_1859beefc.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>clip_f81fd3144.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>clip_10ca32e2a.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>clip_68f14da2e.wav</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>clip_a6046ae7e.wav</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158508</th>\n",
       "      <td>clip_6f801eaed.wav</td>\n",
       "      <td>go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158509</th>\n",
       "      <td>clip_c438e6962.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158510</th>\n",
       "      <td>clip_527621afe.wav</td>\n",
       "      <td>on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158511</th>\n",
       "      <td>clip_f8b9dfcc7.wav</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158512</th>\n",
       "      <td>clip_72e4c67ea.wav</td>\n",
       "      <td>go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158513</th>\n",
       "      <td>clip_01c246fd5.wav</td>\n",
       "      <td>on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158514</th>\n",
       "      <td>clip_b18c3acfb.wav</td>\n",
       "      <td>go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158515</th>\n",
       "      <td>clip_a314d002b.wav</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158516</th>\n",
       "      <td>clip_af435863b.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158517</th>\n",
       "      <td>clip_fe8253850.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158518</th>\n",
       "      <td>clip_ffc557cfc.wav</td>\n",
       "      <td>go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158519</th>\n",
       "      <td>clip_22bf70211.wav</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158520</th>\n",
       "      <td>clip_36aaada51.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158521</th>\n",
       "      <td>clip_119bad96a.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158522</th>\n",
       "      <td>clip_6b989655b.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158523</th>\n",
       "      <td>clip_702f3eae6.wav</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158524</th>\n",
       "      <td>clip_3254d8c14.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158525</th>\n",
       "      <td>clip_21bbfc938.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158526</th>\n",
       "      <td>clip_f71a75a2d.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158527</th>\n",
       "      <td>clip_8591d7073.wav</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158528</th>\n",
       "      <td>clip_d9bf91d8d.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158529</th>\n",
       "      <td>clip_786f35d38.wav</td>\n",
       "      <td>go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158530</th>\n",
       "      <td>clip_e0b72e550.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158531</th>\n",
       "      <td>clip_9341d49f2.wav</td>\n",
       "      <td>silence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158532</th>\n",
       "      <td>clip_3f26932c8.wav</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158533</th>\n",
       "      <td>clip_189edd540.wav</td>\n",
       "      <td>off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158534</th>\n",
       "      <td>clip_7466893f8.wav</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158535</th>\n",
       "      <td>clip_064771499.wav</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158536</th>\n",
       "      <td>clip_ffc6024a6.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158537</th>\n",
       "      <td>clip_35e726bae.wav</td>\n",
       "      <td>silence</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>158538 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     fname    label\n",
       "0       clip_667ea2de0.wav      yes\n",
       "1       clip_4e1d2a516.wav       go\n",
       "2       clip_4746c1f34.wav  unknown\n",
       "3       clip_016404de8.wav       on\n",
       "4       clip_dcb5708d7.wav       no\n",
       "5       clip_af77b5fb6.wav       on\n",
       "6       clip_a78b014de.wav  unknown\n",
       "7       clip_25f0146ae.wav  silence\n",
       "8       clip_f109a09ec.wav     down\n",
       "9       clip_160d3ee69.wav  silence\n",
       "10      clip_f07c5d227.wav     left\n",
       "11      clip_fb1d0cdb0.wav       go\n",
       "12      clip_d9baf3376.wav    right\n",
       "13      clip_761388cd6.wav       up\n",
       "14      clip_0b0070730.wav       no\n",
       "15      clip_0033db83b.wav  unknown\n",
       "16      clip_2c1e066c9.wav  silence\n",
       "17      clip_542857c3e.wav  unknown\n",
       "18      clip_54a4ab635.wav     down\n",
       "19      clip_0ee7bd486.wav  unknown\n",
       "20      clip_82509c6f5.wav      off\n",
       "21      clip_ca40c9a3c.wav       on\n",
       "22      clip_262464bbc.wav  unknown\n",
       "23      clip_5d5805dda.wav     left\n",
       "24      clip_d27c6b701.wav       no\n",
       "25      clip_1859beefc.wav  unknown\n",
       "26      clip_f81fd3144.wav  unknown\n",
       "27      clip_10ca32e2a.wav  unknown\n",
       "28      clip_68f14da2e.wav       up\n",
       "29      clip_a6046ae7e.wav      yes\n",
       "...                    ...      ...\n",
       "158508  clip_6f801eaed.wav       go\n",
       "158509  clip_c438e6962.wav  unknown\n",
       "158510  clip_527621afe.wav       on\n",
       "158511  clip_f8b9dfcc7.wav     down\n",
       "158512  clip_72e4c67ea.wav       go\n",
       "158513  clip_01c246fd5.wav       on\n",
       "158514  clip_b18c3acfb.wav       go\n",
       "158515  clip_a314d002b.wav     down\n",
       "158516  clip_af435863b.wav  unknown\n",
       "158517  clip_fe8253850.wav  unknown\n",
       "158518  clip_ffc557cfc.wav       go\n",
       "158519  clip_22bf70211.wav    right\n",
       "158520  clip_36aaada51.wav  unknown\n",
       "158521  clip_119bad96a.wav  unknown\n",
       "158522  clip_6b989655b.wav  unknown\n",
       "158523  clip_702f3eae6.wav       no\n",
       "158524  clip_3254d8c14.wav  unknown\n",
       "158525  clip_21bbfc938.wav  unknown\n",
       "158526  clip_f71a75a2d.wav  unknown\n",
       "158527  clip_8591d7073.wav      yes\n",
       "158528  clip_d9bf91d8d.wav  unknown\n",
       "158529  clip_786f35d38.wav       go\n",
       "158530  clip_e0b72e550.wav  unknown\n",
       "158531  clip_9341d49f2.wav  silence\n",
       "158532  clip_3f26932c8.wav       up\n",
       "158533  clip_189edd540.wav      off\n",
       "158534  clip_7466893f8.wav     left\n",
       "158535  clip_064771499.wav       no\n",
       "158536  clip_ffc6024a6.wav  unknown\n",
       "158537  clip_35e726bae.wav  silence\n",
       "\n",
       "[158538 rows x 2 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submissionDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
