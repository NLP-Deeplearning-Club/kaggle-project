{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "import re\n",
    "from glob import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 57929 train and 6798 val samples\n"
     ]
    }
   ],
   "source": [
    "DATADIR = './dataset'  # unzipped train and test data\n",
    "ALL_LABELS = ('bed bird cat dog down eight five four go ' +\n",
    "                   'happy house left marvin name.txt nine no ' +\n",
    "                   'off on one right seven sheila six stop three ' +\n",
    "                   'tree two up wow yes zero silence').split()\n",
    "num_labels = len(ALL_LABELS)\n",
    "id2name = {i: name for i, name in enumerate(ALL_LABELS)}\n",
    "name2id = {name: i for i, name in id2name.items()}\n",
    "\n",
    "def load_data(data_dir):\n",
    "    pattern = re.compile(\"(.+\\/)?(\\w+)\\/([^_]+)_.+wav\")\n",
    "    all_files = glob(os.path.join(data_dir, 'train/audio/*/*wav'))\n",
    "\n",
    "    with open(os.path.join(data_dir, 'train/validation_list.txt'), 'r') as fin:\n",
    "        validation_files = fin.readlines()\n",
    "    valset = set()\n",
    "    for entry in validation_files:\n",
    "        r = re.match(pattern, entry)\n",
    "        if r:\n",
    "            valset.add(r.group(3))\n",
    "\n",
    "    possible = set(ALL_LABELS)\n",
    "    train, val = [], []\n",
    "    for entry in all_files:\n",
    "        r = re.match(pattern, entry)\n",
    "        if r:\n",
    "            label, uid = r.group(2), r.group(3)\n",
    "            if label == '_background_noise_':\n",
    "                label = 'silence'\n",
    "            if label not in possible:\n",
    "                label = 'unknown'\n",
    "\n",
    "            label_id = name2id[label]\n",
    "\n",
    "            sample = (label_id, uid, entry)\n",
    "            if uid in valset:\n",
    "                val.append(sample)\n",
    "            else:\n",
    "                train.append(sample)\n",
    "\n",
    "    print('There are {} train and {} val samples'.format(len(train), len(val)))\n",
    "    return train, val\n",
    "\n",
    "train, val = load_data(DATADIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "import python_speech_features\n",
    "\n",
    "def mfcc(audio, sample_rate, **kwargs):\n",
    "    \"\"\"从读出的音频数据中算出mfcc,具体可以看python_speech_features的文档\n",
    "\n",
    "    Parameters:\n",
    "        audio (np.ndarray): - 指明音频的振幅序列\n",
    "        sample_rate (int): - 指明抽样率\n",
    "        numcep (int): - 指明返回的倒数数量,默认为13\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: - mfcc强度(二维)组成的元组,shape为(times.shape,numcep)\n",
    "    \"\"\"\n",
    "    return python_speech_features.mfcc(audio, sample_rate, **kwargs)\n",
    "\n",
    "\n",
    "def mfcc_from_path(record_path, **kwargs):\n",
    "    \"\"\"从音频文件读取出mfcc\n",
    "\n",
    "    Parameters:\n",
    "        record_path (Union[pathlib.Path,str]): - 指明音频的路径\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: - mfcc强度(二维)组成的元组,shape为(times.shape,numcep)\n",
    "    \"\"\"\n",
    "    sample_rate, samples = wavfile.read(str(record_path))\n",
    "    return mfcc(samples, sample_rate,**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Transformed Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 0/57929.\n",
      "Loading 2000/57929.\n",
      "Loading 4000/57929.\n",
      "Loading 6000/57929.\n",
      "Loading 8000/57929.\n",
      "Loading 10000/57929.\n",
      "Loading 12000/57929.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/scipy/io/wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  WavFileWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 14000/57929.\n",
      "Loading 16000/57929.\n",
      "Loading 18000/57929.\n",
      "Loading 20000/57929.\n",
      "Loading 22000/57929.\n",
      "Loading 24000/57929.\n",
      "Loading 26000/57929.\n",
      "Loading 28000/57929.\n",
      "Loading 30000/57929.\n",
      "Loading 32000/57929.\n",
      "Loading 34000/57929.\n",
      "Loading 36000/57929.\n",
      "Loading 38000/57929.\n",
      "Loading 40000/57929.\n",
      "Loading 42000/57929.\n",
      "Loading 44000/57929.\n",
      "Loading 46000/57929.\n",
      "Loading 48000/57929.\n",
      "Loading 50000/57929.\n",
      "Loading 52000/57929.\n",
      "Loading 54000/57929.\n",
      "Loading 56000/57929.\n",
      "Finish loading train data\n",
      "Input Shape:  (57929, 99, 13)\n",
      "Output Shape:  (57929, 32)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "train = np.array(train)\n",
    "np.random.shuffle(train)\n",
    "train = train.tolist()\n",
    "\n",
    "size = len(train)\n",
    "\n",
    "trainX = np.zeros( (len(train),99,13) )\n",
    "trainY = []\n",
    "\n",
    "for i, (label_id,uid,fname) in enumerate(train):\n",
    "    try:\n",
    "        x = mfcc_from_path(fname).tolist()\n",
    "        # ---------------------------------------\n",
    "        # Preprocess the train input sequence\n",
    "        # - Padding 0.0 at the end if length < 99\n",
    "        # - If longer than 99, choose only the first 99 items\n",
    "        # ---------------------------------------\n",
    "        while len(x) < 99:\n",
    "            x.append([0.0]*13)\n",
    "        if len(x) > 99:\n",
    "            x = x[0:99]\n",
    "        trainX[i] = x\n",
    "        trainY.append(label_id)\n",
    "        if i % 2000 == 0:\n",
    "            print(\"Loading {}/{}.\".format(i, size))\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        \n",
    "print(\"Finish loading train data\")\n",
    "trainY = to_categorical(trainY, num_classes=num_labels)\n",
    "\n",
    "print(\"Input Shape: \",  trainX.shape)\n",
    "print(\"Output Shape: \", trainY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 0/6798.\n",
      "Loading 2000/6798.\n",
      "Loading 4000/6798.\n",
      "Loading 6000/6798.\n",
      "Finish loading val data.\n",
      "Input Shape:  (6798, 99, 13)\n",
      "Output Shape:  (6798, 32)\n"
     ]
    }
   ],
   "source": [
    "valX = np.zeros( (len(val),99,13) )\n",
    "valY = []\n",
    "\n",
    "size = len(val)\n",
    "\n",
    "for i, (label_id,uid,fname) in enumerate(val):\n",
    "    try:\n",
    "        x = mfcc_from_path(fname).tolist()\n",
    "        # ---------------------------------------\n",
    "        # Preprocess the train input sequence\n",
    "        # - Padding 0.0 at the end if length < 99\n",
    "        # - If longer than 99, choose only the first 99 items\n",
    "        # ---------------------------------------\n",
    "        while len(x) < 99:\n",
    "            x.append([0.0]*13)\n",
    "        if len(x) > 99:\n",
    "            x = x[0:99]\n",
    "        valX[i] = x\n",
    "        valY.append(label_id)\n",
    "        if i % 2000 == 0:\n",
    "            print(\"Loading {}/{}.\".format(i, size))\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        \n",
    "print(\"Finish loading val data.\")\n",
    "valY = to_categorical(valY, num_classes=num_labels)\n",
    "\n",
    "print(\"Input Shape: \",  valX.shape)\n",
    "print(\"Output Shape: \", valY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, GRU, SimpleRNN, GlobalAveragePooling1D, AveragePooling1D, Activation\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.layers import merge\n",
    "from keras.layers.core import *\n",
    "from keras.models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:24: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/anaconda/lib/python3.6/site-packages/Keras-2.0.8-py3.6.egg/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:41: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    }
   ],
   "source": [
    "max_len = 99\n",
    "embed_dim = 13\n",
    "\n",
    "MODEL_TYPE = \"lstm\"\n",
    "ATTENTION = True\n",
    "LEARN_RATE = 0.001\n",
    "BATCH_SIZE = 20\n",
    "INPUT_SHAPE = [max_len, embed_dim]\n",
    "EPOCHE = 15\n",
    "NUM_HIDDEN = 100\n",
    "TIME_STEPS = max_len\n",
    "SINGLE_ATTENTION_VECTOR = False\n",
    "\n",
    "def attention_3d_block(inputs):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Reshape((input_dim, TIME_STEPS))(a)\n",
    "    a = Dense(TIME_STEPS, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n",
    "    return output_attention_mul\n",
    "\n",
    "inputs = Input(shape=(INPUT_SHAPE))\n",
    "# RNN Layer\n",
    "if MODEL_TYPE == 'rnn':\n",
    "    rnn_out = SimpleRNN(NUM_HIDDEN, return_sequences=True)(inputs)\n",
    "elif MODEL_TYPE == 'gru':\n",
    "    rnn_out = GRU(NUM_HIDDEN, return_sequences=True)(inputs)\n",
    "elif MODEL_TYPE == 'lstm':\n",
    "    rnn_out = LSTM(NUM_HIDDEN, return_sequences=True)(inputs)\n",
    "else:\n",
    "    raise NameError(\"Unsupported model type\")\n",
    "# Attention Layer\n",
    "attention_mul = attention_3d_block(rnn_out)\n",
    "attention_mul = Flatten()(attention_mul)\n",
    "output = Dense(num_labels, activation='softmax')(attention_mul)\n",
    "model = Model(input=[inputs], output=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 99, 13)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 99, 100)      45600       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 100, 99)      0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 100, 99)      0           permute_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100, 99)      9900        reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_vec (Permute)         (None, 99, 100)      0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_mul (Merge)           (None, 99, 100)      0           lstm_1[0][0]                     \n",
      "                                                                 attention_vec[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 9900)         0           attention_mul[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 32)           316832      flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 372,332\n",
      "Trainable params: 372,332\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "# Early Stopping\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=2, mode='auto')\n",
    "\n",
    "# Reduce lr\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=1, min_lr=0.001)\n",
    "\n",
    "callbacks = [early_stop, reduce_lr]\n",
    "\n",
    "# Train\n",
    "model_info = model.fit(trainX, trainY, \n",
    "          epochs=EPOCHE, batch_size=BATCH_SIZE, \n",
    "          verbose=2, validation_data=(valX, valY),\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_model_history(model_history):\n",
    "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
    "    # summarize history for accuracy\n",
    "    axs[0].plot(range(1,len(model_history.history['acc'])+1),model_history.history['acc'])\n",
    "    axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])\n",
    "    axs[0].set_title('Model Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])/10)\n",
    "    axs[0].legend(['train', 'val'], loc='best')\n",
    "    # summarize history for loss\n",
    "    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
    "    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
    "    axs[1].legend(['train', 'val'], loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "plot_model_history(model_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob    \n",
    "import pandas as pd\n",
    "\n",
    "test_files = glob.glob(\"./dataset/test/audio/*.wav\")\n",
    "test_filenames = [f.split(\"/\")[-1] for f in test_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 0/158538\n",
      "Predicting 3000/158538\n",
      "Predicting 6000/158538\n",
      "Predicting 9000/158538\n",
      "Predicting 12000/158538\n",
      "Predicting 15000/158538\n",
      "Predicting 18000/158538\n",
      "Predicting 21000/158538\n",
      "Predicting 24000/158538\n",
      "Predicting 27000/158538\n",
      "Predicting 30000/158538\n",
      "Predicting 33000/158538\n",
      "Predicting 36000/158538\n",
      "Predicting 39000/158538\n",
      "Predicting 42000/158538\n",
      "Predicting 45000/158538\n",
      "Predicting 48000/158538\n",
      "Predicting 51000/158538\n",
      "Predicting 54000/158538\n",
      "Predicting 57000/158538\n",
      "Predicting 60000/158538\n",
      "Predicting 63000/158538\n",
      "Predicting 66000/158538\n",
      "Predicting 69000/158538\n",
      "Predicting 72000/158538\n",
      "Predicting 75000/158538\n",
      "Predicting 78000/158538\n",
      "Predicting 81000/158538\n",
      "Predicting 84000/158538\n",
      "Predicting 87000/158538\n",
      "Predicting 90000/158538\n",
      "Predicting 93000/158538\n",
      "Predicting 96000/158538\n",
      "Predicting 99000/158538\n",
      "Predicting 102000/158538\n",
      "Predicting 105000/158538\n",
      "Predicting 108000/158538\n",
      "Predicting 111000/158538\n",
      "Predicting 114000/158538\n",
      "Predicting 117000/158538\n",
      "Predicting 120000/158538\n",
      "Predicting 123000/158538\n",
      "Predicting 126000/158538\n",
      "Predicting 129000/158538\n",
      "Predicting 132000/158538\n",
      "Predicting 135000/158538\n",
      "Predicting 138000/158538\n",
      "Predicting 141000/158538\n",
      "Predicting 144000/158538\n",
      "Predicting 147000/158538\n",
      "Predicting 150000/158538\n",
      "Predicting 153000/158538\n",
      "Predicting 156000/158538\n",
      "Finish predicting.....\n",
      "Writing to file.....\n"
     ]
    }
   ],
   "source": [
    "submission = []\n",
    "\n",
    "possible_labels = ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'silence']\n",
    "\n",
    "num_test = len(test_files)\n",
    "\n",
    "for i in range(num_test):\n",
    "    test_wav = mfcc_from_path(test_files[i])[np.newaxis,:,:]\n",
    "    label = id2name[np.argmax( model.predict(test_wav) )]\n",
    "    if label not in possible_labels:\n",
    "        label = 'unknown' \n",
    "    submission.append([test_filenames[i], label])\n",
    "    if i % 3000 == 0:\n",
    "        print(\"Predicting {}/{}\".format(i, num_test))\n",
    "\n",
    "print(\"Finish predicting.....\")\n",
    "\n",
    "submissionDF = pd.DataFrame(submission, columns=['fname','label'])\n",
    "\n",
    "print(\"Writing to file.....\")\n",
    "submissionDF.to_csv(\"submission-lstm-attention.csv\", index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
